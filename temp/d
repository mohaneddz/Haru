# voice.py
import pyaudio
import numpy as np
import requests
import json
import base64
import pygame
import tempfile
import time
import threading
import os
import torch
import soundfile as sf
import asyncio
import websockets # For WebSocket server
from websockets.server import serve
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
from kokoro import KPipeline
from flask import Flask, Response # For readiness server
from flask_cors import CORS # Import CORS
import logging

# Suppress Flask and Werkzeug startup messages
werkzeug_logger = logging.getLogger('werkzeug')
werkzeug_logger.setLevel(logging.ERROR)
# Suppress websockets verbose output
logging.getLogger("websockets.server").setLevel(logging.ERROR)
logging.getLogger("websockets.protocol").setLevel(logging.ERROR)


class VoiceClient:
    MODEL_ID = "openai/whisper-large-v3-turbo"  # Use a class constant for model id
    READINESS_PORT = 7777
    WEBSOCKET_PORT = 8765

    def __init__(self, server_url="http://localhost:8080/chat"):
        self.server_url = server_url
        self.history = []
        
        # Audio I/O Configuration
        self.RATE = 16000
        self.CHUNK = int(self.RATE * 0.1)  # 100ms
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1
        
        # VAD Configuration
        self.SILENCE_THRESHOLD = 0.003
        self.SILENCE_DURATION = 1
        self.MIN_RECORDING_LENGTH = 0.5
        
        # State
        self.recording_buffer = []
        self.last_voice_time = time.time()
        self.is_recording = False
        self.is_playing = False
        self.is_thinking = False
        self.running = True

        # Synchronization for model loading and server readiness
        self.models_loaded_event = threading.Event()
        
        # WebSocket server variables
        self.websocket_server_task = None
        self.websocket_loop = None
        self.connected_ws_clients = set() # Store connected WebSocket clients

        # Start servers in separate daemon threads immediately
        self._start_websocket_server_thread()
        self._start_readiness_server_thread()

        # --- Model Loading (happens in main thread) ---
        # These will block the main thread until loaded
        self.stt_model, self.stt_processor = self._load_stt_model()
        self.tts_pipeline, self.voice_tensor = self._load_tts_model()

        # Signal that models are loaded AFTER they've successfully loaded
        if self.stt_model and self.tts_pipeline:
            self.models_loaded_event.set()
            print("‚úÖ All models loaded. Signaling readiness.")
        else:
            print("‚ùå Some models failed to load. Readiness signal not sent.")
        
        # --- Audio System Initialization ---
        self.p = pyaudio.PyAudio()
        pygame.mixer.init(frequency=24000, size=-16, channels=1, buffer=512)
        
        # Open audio stream for microphone input
        self.stream = self.p.open(
            format=self.FORMAT, channels=self.CHANNELS, rate=self.RATE,
            input=True, frames_per_buffer=self.CHUNK, stream_callback=self.audio_callback
        )
        self.stream.start_stream()

    # --- Server Setup ---
    def _start_readiness_server_thread(self):
        """Starts the Flask readiness server in a separate daemon thread."""
        app = Flask(__name__)
        CORS(app) # Enable CORS for all origins by default on this app
        app.logger.setLevel(logging.ERROR) # Suppress Flask logs for this instance too

        @app.route('/wait')
        def wait_for_ready():
            def generate():
                print(f"Waiting for models to load on /wait endpoint...")
                self.models_loaded_event.wait() # Block until models are loaded
                print("Models loaded. Sending 'ok' signal to frontend.")
                yield "event: message\ndata: ok\n\n"
            return Response(generate(), mimetype='text/event-stream')

        def run_flask():
            # Use a basic HTTP server provided by Flask, suppress output
            try:
                # Use threaded=True for multiple connections (e.g., frontend retries)
                app.run(port=self.READINESS_PORT, debug=False, use_reloader=False, threaded=True)
            except OSError as e:
                print(f"‚ùå Readiness server failed to start on port {self.READINESS_PORT}: {e}")
                print("   Is another process already using this port? Frontend will not receive ready signal.")

        readiness_thread = threading.Thread(target=run_flask, daemon=True)
        readiness_thread.start()
        print(f"üì° Readiness server (HTTP SSE) starting on port {self.READINESS_PORT}...")


    def _start_websocket_server_thread(self):
        """Starts the WebSocket server in a separate daemon thread."""
        def run_websocket_server_loop():
            self.websocket_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.websocket_loop)

            async def handler(websocket):
                print(f"Frontend client connected via WebSocket: {websocket.remote_address}")
                self.connected_ws_clients.add(websocket)
                try:
                    await websocket.wait_closed() # Keep connection open until client closes it
                except websockets.exceptions.ConnectionClosedOK:
                    pass # Expected on normal close
                except Exception as e:
                    print(f"WebSocket handler error for {websocket.remote_address}: {e}")
                finally:
                    if websocket in self.connected_ws_clients:
                        self.connected_ws_clients.remove(websocket)
                        print(f"Frontend client disconnected: {websocket.remote_address}")

            # Start the WebSocket server coroutine
            self.websocket_server_task = self.websocket_loop.create_task(
                serve(handler, "localhost", self.WEBSOCKET_PORT)
            )
            
            try:
                self.websocket_loop.run_forever() # Run the event loop indefinitely
            except asyncio.CancelledError:
                print("WebSocket server loop cancelled.")
            except Exception as e:
                print(f"An unexpected error occurred in WebSocket server: {e}")

        websocket_thread = threading.Thread(target=run_websocket_server_loop, daemon=True)
        websocket_thread.start()
        print(f"üåê WebSocket server starting on port {self.WEBSOCKET_PORT}...")

    async def _send_to_frontend_async(self, message_type: str, data: any):
        """Asynchronously sends a JSON message to all connected frontend clients."""
        if not self.connected_ws_clients:
            return

        message = json.dumps({"type": message_type, "data": data})
        
        # Send to all connected clients, handle disconnections
        disconnected_clients = set()
        for client in list(self.connected_ws_clients): # Iterate over a copy to allow modification
            try:
                await client.send(message)
            except websockets.exceptions.ConnectionClosed:
                disconnected_clients.add(client)
            except Exception as e:
                print(f"Error sending to WebSocket client {client.remote_address}: {e}")
                disconnected_clients.add(client)
        self.connected_ws_clients.difference_update(disconnected_clients)

    def _send_to_frontend(self, message_type: str, data: any):
        """Sends a JSON message to frontend, safe for calling from synchronous threads."""
        if self.websocket_loop and self.websocket_loop.is_running():
            try:
                # Schedule the async function to run on the WebSocket event loop
                asyncio.run_coroutine_threadsafe(
                    self._send_to_frontend_async(message_type, data), 
                    self.websocket_loop
                )
            except Exception as e:
                print(f"Error scheduling message '{message_type}' for frontend: {e}")
        else:
            # This can happen if the websocket server failed to start or is shutting down
            # print(f"Warning: WebSocket loop not running. Could not send '{message_type}' to frontend.")
            pass # Suppress warning for now, it's expected during shutdown or if server failed.


    def _load_stt_model(self):
        """Loads the Whisper STT model and processor."""
        print("Loading STT model (Whisper)...")
        try:
            device = "cuda:0" if torch.cuda.is_available() else "cpu"
            torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
            
            model = AutoModelForSpeechSeq2Seq.from_pretrained(
                self.MODEL_ID, torch_dtype=torch_dtype
            ).to(device)
            processor = AutoProcessor.from_pretrained(self.MODEL_ID)
            print("‚úÖ STT model loaded successfully.")
            return model, processor
        except Exception as e:
            print(f"‚ùå Failed to load STT model: {e}")
            return None, None

    def _load_tts_model(self):
        """Loads the Kokoro TTS model and voice."""
        print("Loading TTS model (Kokoro)...")
        try:
            pipeline = KPipeline(lang_code='a')
            # Ensure 'voices/af_nicole.pt' exists relative to this script
            voice_tensor = torch.load("voices/af_nicole.pt", weights_only=True) 
            print("‚úÖ TTS model loaded successfully.")
            return pipeline, voice_tensor
        except Exception as e:
            print(f"‚ùå Failed to load TTS model: {e}")
            return None, None
            
    def transcribe_audio(self, audio_data):
        """Transcribes raw audio bytes using the local Whisper model."""
        if not self.stt_model:
            print("‚ùå STT model not available.")
            return ""
        try:
            print("üé§ Transcribing...")
            audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            
            inputs = self.stt_processor(
                audio_array, sampling_rate=self.RATE, return_tensors="pt"
            )
            # Ensure inputs are on the correct device and dtype
            inputs = inputs.to(self.stt_model.device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)

            with torch.no_grad():
                predicted_ids = self.stt_model.generate(inputs.input_features, max_new_tokens=128)
            
            transcription = self.stt_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].strip()
            print(f"üó£Ô∏è You: {transcription}")
            self._send_to_frontend("transcript", transcription) # Send transcript to frontend
            return transcription
        except Exception as e:
            print(f"‚ùå Transcription error: {e}")
            self._send_to_frontend("error", f"Transcription error: {e}")
            return ""

    def synthesize_speech(self, text):
        """Synthesizes speech from text using the local Kokoro model."""
        if not self.tts_pipeline:
            print("‚ùå TTS model not available.")
            return None
        try:
            print("üîä Synthesizing audio...")
            self._send_to_frontend("status", "Synthesizing...")
            generator = self.tts_pipeline(text, voice=self.voice_tensor, speed=1.0)
            
            audio_segments = [audio for _, _, audio in generator]
            if not audio_segments:
                print("‚ö†Ô∏è No audio segments generated by TTS.")
                return None
                
            full_audio = np.concatenate(audio_segments)
            
            # Save to a temporary WAV file for Pygame playback
            buffer = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
            sf.write(buffer.name, full_audio, 24000) # Kokoro outputs at 24000 Hz
            buffer.close()
            return buffer.name
        except Exception as e:
            print(f"‚ùå TTS error: {e}")
            self._send_to_frontend("error", f"TTS error: {e}")
            return None

    def audio_callback(self, in_data, frame_count, time_info, status):
        """Audio input callback for PyAudio stream."""
        # Only process audio input if not playing or thinking.
        # This prevents new recordings from starting while the assistant is responding or processing.
        if self.is_playing or self.is_thinking:
            return (None, pyaudio.paContinue)

        audio_array = np.frombuffer(in_data, dtype=np.int16).astype(np.float32) / 32768.0
        rms = np.sqrt(np.mean(audio_array**2))
        current_time = time.time()
        
        if rms > self.SILENCE_THRESHOLD:
            if not self.is_recording:
                print("üî¥ Recording started...")
                self._send_to_frontend("status", "Recording...") # Update frontend status
                self.is_recording = True
                self.recording_buffer = []
            self.last_voice_time = current_time
            self.recording_buffer.append(in_data)
        elif self.is_recording:
            self.recording_buffer.append(in_data) # Keep buffering for the duration of silence
            if current_time - self.last_voice_time >= self.SILENCE_DURATION:
                self.is_recording = False
                total_duration = len(self.recording_buffer) * self.CHUNK / self.RATE
                if total_duration >= self.MIN_RECORDING_LENGTH:
                    audio_data = b''.join(self.recording_buffer)
                    # Process interaction in a new thread to keep audio callback non-blocking
                    threading.Thread(target=self.process_interaction, args=(audio_data,)).start()
                else:
                    print("‚ö†Ô∏è Recording too short, skipped. Listening...")
                    self._send_to_frontend("status", "Listening...") # Update frontend status
                self.recording_buffer = [] # Clear buffer after processing or skipping

        return (None, pyaudio.paContinue)

    def process_interaction(self, source_data, is_text=False):
        """Handles the full interaction: STT -> LLM -> TTS."""
        self.is_thinking = True
        self._send_to_frontend("status", "Thinking...") # Update frontend status

        user_message = ""
        if is_text:
            user_message = source_data
            print(f"üó£Ô∏è You (text input): {user_message}")
            self._send_to_frontend("transcript", user_message) # Send text input to frontend as transcript
        else:
            user_message = self.transcribe_audio(source_data) # This also sends to frontend
        
        if not user_message:
            print("üé§ Listening...")
            self._send_to_frontend("status", "Listening...") # Update frontend status
            self.is_thinking = False
            return
            
        self.history.append({"role": "user", "content": user_message})

        try:
            print("ü§ñ Assistant: ", end="", flush=True)
            llm_response_full = ""
            
            # Send history *before* this turn
            payload = {"message": user_message, "history": self.history[:-1]} 
            response = requests.post(self.server_url, json=payload, stream=True)
            response.raise_for_status()

            for line in response.iter_lines():
                if self.running is False: # Check if app is shutting down
                    break
                if line.startswith(b'data:'):
                    data_str = line.decode('utf-8')[5:]
                    if data_str:
                        chunk = json.loads(data_str)
                        token = chunk.get("content", "")
                        print(token, end="", flush=True)
                        llm_response_full += token
                        self._send_to_frontend("llm_token", token) # Send token to frontend
                elif line.startswith(b'event: end'):
                    break
            
            print() # Newline after the full response
            self._send_to_frontend("llm_end", None) # Signal LLM response completion

            if llm_response_full:
                self.history.append({"role": "assistant", "content": llm_response_full})
                
                # Keep history manageable (e.g., last 10 turns)
                if len(self.history) > 10:
                    self.history = self.history[-10:]

                audio_file_path = self.synthesize_speech(llm_response_full)
                if audio_file_path:
                    self.play_audio_response(audio_file_path)
            
        except requests.exceptions.ConnectionError:
             print("\n‚ùå CONNECTION ERROR: Could not connect to the LLM server. Is system.py running?")
             self._send_to_frontend("error", "LLM server connection failed.")
        except Exception as e:
            print(f"\n‚ùå An error occurred during interaction: {e}")
            self._send_to_frontend("error", f"Interaction error: {e}")
        finally:
            self.is_thinking = False
            print("üé§ Listening...")
            self._send_to_frontend("status", "Listening...") # Update frontend status after interaction

            
    def play_audio_response(self, file_path):
        """Plays audio from a file path using Pygame and cleans up the temp file."""
        try:
            self.is_playing = True
            self._send_to_frontend("status", "Speaking...") # Update frontend status
            pygame.mixer.music.load(file_path)
            pygame.mixer.music.play()
            while pygame.mixer.music.get_busy() and self.running:
                time.sleep(0.1)
        except pygame.error as e:
            print(f"Pygame audio playback error: {e}")
            self._send_to_frontend("error", f"Audio playback error: {e}")
        except Exception as e:
            print(f"General audio playback error: {e}")
            self._send_to_frontend("error", f"Audio playback error: {e}")
        finally:
            self.is_playing = False
            self._send_to_frontend("status", "Listening...") # Update frontend status after speaking
            try:
                os.unlink(file_path) # Delete the temporary audio file
            except Exception as e:
                print(f"Could not delete temp file {file_path}: {e}")

    def cleanup(self):
        """Performs graceful shutdown of all resources."""
        print("üõë Shutting down VoiceClient...")
        self.running = False # Signal threads to stop

        # Stop PyAudio stream
        if self.stream.is_active():
            self.stream.stop_stream()
        self.stream.close()
        self.p.terminate()
        
        # Stop Pygame mixer
        if pygame.mixer.get_init():
            pygame.mixer.music.stop()
            pygame.mixer.quit()
        
        # Stop WebSocket server gracefully
        if self.websocket_server_task:
            self.websocket_server_task.cancel() # Cancel the server task
            # Give the loop a moment to process cancellation
            if self.websocket_loop and self.websocket_loop.is_running():
                # To truly stop run_forever, need to stop the loop from another thread.
                # A simple stop() is not directly thread-safe from outside the loop.
                # However, cancelling the task and allowing the daemon thread to exit naturally
                # is usually sufficient for graceful shutdown.
                pass 
            self.websocket_server_task = None
        
        # Small delay to allow threads to clean up
        time.sleep(0.5) 
        print("‚úÖ Cleanup complete")


    def run(self):
        print("\nüé§ Voice Client Ready!")
        print("   - Speak and pause for 1 second to send a message.")
        print("   - Type a message and press Enter to send it as text.")
        print("   - Type 'quit' to exit.\n")
        
        # Initial status update after everything is set up and listening
        self._send_to_frontend("status", "Listening...")

        try:
            while self.running:
                # This input() call is blocking the main thread.
                # Voice interaction happens in audio_callback and spawned threads.
                # This loop is primarily for handling manual text input or 'quit'.
                user_input = input() 
                if user_input.lower() == 'quit':
                    break
                if user_input:
                    # Don't process if another interaction is already happening
                    if not self.is_thinking:
                        threading.Thread(target=self.process_interaction, args=(user_input, True)).start()
                    else:
                        print("‚ö†Ô∏è Please wait for the current response to finish.")
        except KeyboardInterrupt:
            # Allow graceful shutdown on Ctrl+C
            pass
        finally:
            self.cleanup()

if __name__ == "__main__":
    client = VoiceClient()
    client.run()